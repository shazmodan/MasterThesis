\section{History}
Evaluating \cite{WinNT} \cite{Nobody06} code automatically is nothing new. The earliest known system was built in 1960 by Hollingsworth (referens Hollingsworth). This system was used in an introductory course in Algol programming. The results from using this student-system approach rather than the traditional student-teacher was that it cut costs considerably for the staff since the time they needed to grade the students work was reduced by as much as one third. The students themselves also spent less time, since they were able to have their work graded immidiately instead of waiting for a teacher to do it. This system also made it possible to considerably increase the number of students taking the course. It did, however, have some shortcomings. For instance, a student's program could modify the grader program, making cheating possible. 

An article from 2005 (referens p1-douce) describes three generations of automatic graders. The first generation systems were those regarded as being built and/or used in the 1960's and 1970's. Unsurprisingly, they used code that were close to pure machine code and some even used punched cards. In order to make them work, it was necessary to modify both compiler and operating system. 

The second generation systems (1980-2000) introduced script-based tools. These involved various verification schemes and also asserted that the code was written in a certain way/style (decided by the teacher). Typically these graders involved command-line GUIs. Languages like C and Java were used extensively.

The third generation (2000-) differ from the second generation systems primarily in two ways. One is that they mostly use web based GUIs. The other is that they often included a plagiarism detection system, since students sometimes shared code amongst each other. There were some minor issues among these detection systems (referens ISECON2006)(referens p1-douce). If the programming task was too simple or if a lecturer had been excessively thorough when describing the homework, the submissions would tend to be very much alike and thus picked up by the plagiarism detection system. This made it somewhat difficult to distinguish between real plagiarism and the false positives. 